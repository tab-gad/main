{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65b2ff3a",
   "metadata": {},
   "source": [
    "# Tabular GAD 실험 (ADBench + TabPFN Embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad9b04",
   "metadata": {},
   "source": [
    "##### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "502845e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tabpfn_extensions import TabPFNClassifier\n",
    "from tabpfn_extensions.embedding import TabPFNEmbedding\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a7a086",
   "metadata": {},
   "source": [
    "##### 데이터셋 전처리\n",
    "- 결과 분석이나 LLM 컬럼 처리 등에서 컬럼명이 필요하므로 가져오기\n",
    "- TabPFN에서도 A/B 도메인에서 학습하고 C에서 테스트하려면 여러 도메인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b0f375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 4_breastw | X=(683, 9) -> /home/haeylee/main/dataset/export_with_columns\n",
      "[OK] 29_Pima | X=(768, 8) -> /home/haeylee/main/dataset/export_with_columns\n",
      "[OK] 43_WDBC | X=(367, 30) -> /home/haeylee/main/dataset/export_with_columns\n",
      "[OK] 45_wine | X=(129, 13) -> /home/haeylee/main/dataset/export_with_columns\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "ADBENCH_ROOT = Path(\"/home/haeylee/main/dataset\")\n",
    "CLASSICAL_DIR = ADBENCH_ROOT / \"Classical\"\n",
    "OUT_DIR = ADBENCH_ROOT / \"export_with_columns\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Column name maps\n",
    "COLS = {\n",
    "    \"breastw\": [\n",
    "        \"Clump_Thickness\",\"Uniformity_of_Cell_Size\",\"Uniformity_of_Cell_Shape\",\n",
    "        \"Marginal_Adhesion\",\"Single_Epithelial_Cell_Size\",\"Bare_Nuclei\",\n",
    "        \"Bland_Chromatin\",\"Normal_Nucleoli\",\"Mitoses\",\n",
    "    ],\n",
    "    \"Pima\": [\n",
    "        \"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n",
    "        \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\",\n",
    "    ],\n",
    "    \"wine\": [\n",
    "        \"Alcohol\",\"Malicacid\",\"Ash\",\"Alcalinity_of_ash\",\"Magnesium\",\n",
    "        \"Total_phenols\",\"Flavanoids\",\"Nonflavanoid_phenols\",\"Proanthocyanins\",\n",
    "        \"Color_intensity\",\"Hue\",\"OD280_OD315_of_diluted_wines\",\"Proline\",\n",
    "    ],\n",
    "    \"WDBC\": [\n",
    "        \"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\",\n",
    "        \"compactness_mean\",\"concavity_mean\",\"concave_points_mean\",\"symmetry_mean\",\"fractal_dimension_mean\",\n",
    "        \"radius_se\",\"texture_se\",\"perimeter_se\",\"area_se\",\"smoothness_se\",\n",
    "        \"compactness_se\",\"concavity_se\",\"concave_points_se\",\"symmetry_se\",\"fractal_dimension_se\",\n",
    "        \"radius_worst\",\"texture_worst\",\"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\n",
    "        \"compactness_worst\",\"concavity_worst\",\"concave_points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Datasets to export\n",
    "TARGETS = [\n",
    "    (4,  \"breastw\"),\n",
    "    (29, \"Pima\"),\n",
    "    (43, \"WDBC\"),\n",
    "    (45, \"wine\"),\n",
    "]\n",
    "\n",
    "def make_columns(d: int, preferred: list[str] | None) -> list[str]:\n",
    "\n",
    "    if preferred is not None and len(preferred) == d:\n",
    "        return preferred\n",
    "    return [f\"f{i+1}\" for i in range(d)]\n",
    "\n",
    "def export_one(ds_id: int, name: str):\n",
    "    npz_path = CLASSICAL_DIR / f\"{ds_id}_{name}.npz\"\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "    X = data[\"X\"]\n",
    "    y = data[\"y\"].astype(int)\n",
    "\n",
    "    cols = make_columns(X.shape[1], COLS.get(name))\n",
    "    df = pd.DataFrame(X, columns=cols)\n",
    "    df[\"is_anomaly\"] = y\n",
    "\n",
    "    df.to_parquet(OUT_DIR / f\"{ds_id}_{name}.parquet\", index=False)\n",
    "    df.to_csv(OUT_DIR / f\"{ds_id}_{name}.csv\", index=False)\n",
    "\n",
    "    print(f\"[OK] {ds_id}_{name} | X={X.shape} -> {OUT_DIR}\")\n",
    "\n",
    "for ds_id, name in TARGETS:\n",
    "    export_one(ds_id, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a43824fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(367, 31)\n",
      "Index(['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
      "       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
      "       'concave_points_mean'],\n",
      "      dtype='object')\n",
      "is_anomaly\n",
      "0    357\n",
      "1     10\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>is_anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.30</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.1082</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.1683</td>\n",
       "      <td>0.08751</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>...</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.3</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.6110</td>\n",
       "      <td>0.6335</td>\n",
       "      <td>0.2024</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.09876</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.87</td>\n",
       "      <td>16.67</td>\n",
       "      <td>98.64</td>\n",
       "      <td>682.5</td>\n",
       "      <td>0.1162</td>\n",
       "      <td>0.1649</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>0.08923</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.06768</td>\n",
       "      <td>...</td>\n",
       "      <td>27.37</td>\n",
       "      <td>127.1</td>\n",
       "      <td>1095.0</td>\n",
       "      <td>0.1878</td>\n",
       "      <td>0.4480</td>\n",
       "      <td>0.4704</td>\n",
       "      <td>0.2027</td>\n",
       "      <td>0.3585</td>\n",
       "      <td>0.10650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.05</td>\n",
       "      <td>19.08</td>\n",
       "      <td>113.40</td>\n",
       "      <td>895.0</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>0.1572</td>\n",
       "      <td>0.1910</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.06325</td>\n",
       "      <td>...</td>\n",
       "      <td>24.89</td>\n",
       "      <td>133.5</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.3934</td>\n",
       "      <td>0.5018</td>\n",
       "      <td>0.2543</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.09061</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        15.30         25.27          102.40      732.4           0.1082   \n",
       "1        14.87         16.67           98.64      682.5           0.1162   \n",
       "2        17.05         19.08          113.40      895.0           0.1141   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave_points_mean  symmetry_mean  \\\n",
       "0            0.1697          0.1683              0.08751         0.1926   \n",
       "1            0.1649          0.1690              0.08923         0.2157   \n",
       "2            0.1572          0.1910              0.10900         0.2131   \n",
       "\n",
       "   fractal_dimension_mean  ...  texture_worst  perimeter_worst  area_worst  \\\n",
       "0                 0.06540  ...          36.71            149.3      1269.0   \n",
       "1                 0.06768  ...          27.37            127.1      1095.0   \n",
       "2                 0.06325  ...          24.89            133.5      1189.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave_points_worst  \\\n",
       "0            0.1641             0.6110           0.6335                0.2024   \n",
       "1            0.1878             0.4480           0.4704                0.2027   \n",
       "2            0.1703             0.3934           0.5018                0.2543   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  is_anomaly  \n",
       "0          0.4027                  0.09876           1  \n",
       "1          0.3585                  0.10650           1  \n",
       "2          0.3109                  0.09061           1  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 예시 봐보기\n",
    "\n",
    "EXPORT_DIR = ADBENCH_ROOT / \"export_with_columns\"\n",
    "\n",
    "def load_exported_df(ds_id: int, name: str, prefer: str = \"parquet\") -> pd.DataFrame:\n",
    "    \"\"\"export_with_columns에서 DataFrame을 읽어오기\"\"\"\n",
    "    p_parq = EXPORT_DIR / f\"{ds_id}_{name}.parquet\"\n",
    "    p_csv  = EXPORT_DIR / f\"{ds_id}_{name}.csv\"\n",
    "\n",
    "    if prefer == \"parquet\" and p_parq.exists():\n",
    "        return pd.read_parquet(p_parq)\n",
    "    if prefer == \"csv\" and p_csv.exists():\n",
    "        return pd.read_csv(p_csv)\n",
    "\n",
    "    # fallback\n",
    "    return pd.read_parquet(p_parq) if p_parq.exists() else pd.read_csv(p_csv)\n",
    "\n",
    "def df_to_xy(df: pd.DataFrame):\n",
    "    \"\"\"DataFrame -> (X, y, feature_names)로 변환\"\"\"\n",
    "    y = df[\"is_anomaly\"].to_numpy().astype(int) \n",
    "    X = df.drop(columns=[\"is_anomaly\"]).to_numpy() # anomaly 라벨 제외한 feature 행렬\n",
    "    feature_names = df.columns.drop(\"is_anomaly\").tolist() # 결과해석용으로 컬럼명도 뽑기\n",
    "    return X, y, feature_names\n",
    "\n",
    "# 예시: WDBC 확인\n",
    "df_wdbc = load_exported_df(43, \"WDBC\", prefer=\"parquet\")\n",
    "print(df_wdbc.shape)\n",
    "print(df_wdbc.columns[:8])\n",
    "print(df_wdbc[\"is_anomaly\"].value_counts())\n",
    "\n",
    "df_wdbc.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9540a17c",
   "metadata": {},
   "source": [
    "### TabPFN (1) 임베딩 기반 파이프라인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ddfe01",
   "metadata": {},
   "source": [
    "- 각 row를 벡터 임베딩으로 만들고, 그 임베딩 위에 anomaly detector 얹어 AD 성능 구하는 코드\n",
    "- TabPFN 임베딩 추출은 `tabpfn-extensions`의 Embeddings extension 그대로 씀\n",
    "- 흐름\n",
    "1) ADBench 데이터 로드\n",
    "2) train/test split\n",
    "3) TabPFNEmbedding으로 H_train, H_test 추출\n",
    "4) 임베딩에 대해 간단한 detector(IsolationForest) 학습 - test에서 anomaly score\n",
    "5) AD 성능 계산 (AUROC / Average Precision)\n",
    "- 라벨을 임베딩 추출에 쓸지말지 둘다 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c11e1",
   "metadata": {},
   "source": [
    "##### TabPFN을 임베딩 추출기로 사용해 row embedding 뽑기\n",
    "- n_fold = 0 : 빠른 대신 덜 robust (vanilla)\n",
    "- n_fold > 0 : CV 기반 임베딩 (default:5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f035df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabPFNEmbedding으로 H_train, H_test 뽑기\n",
    "\n",
    "def tabpfn_extract_embeddings(\n",
    "    X_train, y_train_for_embed, X_test,\n",
    "    n_fold=5,\n",
    "    n_estimators=1,\n",
    "):\n",
    "    \n",
    "    clf = TabPFNClassifier(n_estimators=n_estimators)\n",
    "    embedder = TabPFNEmbedding(tabpfn_clf=clf, n_fold=n_fold)\n",
    "\n",
    "    # n_fold=0일 때만 fit 필요\n",
    "    if n_fold == 0:\n",
    "        embedder.fit(X_train, y_train_for_embed)\n",
    "\n",
    "    H_train = embedder.get_embeddings(X_train, y_train_for_embed, X_test, data_source=\"train\")\n",
    "    H_test  = embedder.get_embeddings(X_train, y_train_for_embed, X_test, data_source=\"test\")\n",
    "    return H_train, H_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39077a25",
   "metadata": {},
   "source": [
    "- 각 데이터셋마다 train/test 한 번 나누고\n",
    "- 같은 split에 대해 \n",
    "    - dummy 버전 임베딩 (라벨 안씀) : H_train_dummy, H_test_dummy\n",
    "    - use_anomaly_labels 버전 임베딩 (라벨 씀) : H_train_lbl, H_test_lbl\n",
    "    두 가지 각각 저장하기\n",
    "- 위 과정을 4개 데이터셋 모두에 대해 반복\n",
    "- 평가 : 저장된 `H_train`, `H_test`, `y_train`, `y_test`를 불러와서 detector(ex.IsolationForest)로 성능평가하기\n",
    "    - in-domain : 같은 데이터셋의 `H_train`으로 학습 --> 같은 데이터셋의 `H_test`로 평가\n",
    "    - cross-domain : A 데이터셋의 `H_train`으로 학습 --> B 데이터셋의 `H_test`로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "329d93ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> New run directory: /home/haeylee/main/dataset/export_with_columns/tabpfn_embeddings/20251211_1628\n"
     ]
    }
   ],
   "source": [
    "EMB_OUT_DIR = EXPORT_DIR / \"tabpfn_embeddings\"\n",
    "EMB_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "RUN_DIR = EMB_OUT_DIR / RUN_ID\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\">>> New run directory:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2574ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings (both modes) for ONE dataset\n",
    "\n",
    "def save_embeddings_for_one_dataset(\n",
    "    ds_id: int,\n",
    "    name: str,\n",
    "    prefer: str = \"parquet\",\n",
    "    test_size: float = 0.5,\n",
    "    seed: int = 42,\n",
    "    n_fold: int = 5,\n",
    "):\n",
    "\n",
    "    # 1) load\n",
    "    df = load_exported_df(ds_id, name, prefer=prefer)\n",
    "    X, y, feature_names = df_to_xy(df)\n",
    "\n",
    "    # 2) split (anomaly ratio 유지)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=seed,\n",
    "        stratify=y # 아노말리 비율 유지\n",
    "    )\n",
    "\n",
    "    # 3) 저장 경로: RUN_DIR / dataset / fold / mode\n",
    "    base_dir = RUN_DIR / name / f\"fold{n_fold}\"\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    metas = {}\n",
    "\n",
    "    # -------------------------\n",
    "    # (A) dummy\n",
    "    # -------------------------\n",
    "    mode = \"dummy\"\n",
    "    save_dir = base_dir / mode\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    y_for_embed = np.zeros_like(y_train)\n",
    "\n",
    "    H_train, H_test = tabpfn_extract_embeddings(\n",
    "        X_train, y_for_embed, X_test,\n",
    "        n_fold=n_fold,\n",
    "        n_estimators=1\n",
    "    )\n",
    "\n",
    "    np.save(save_dir / \"H_train.npy\", H_train)\n",
    "    np.save(save_dir / \"H_test.npy\",  H_test)\n",
    "    np.save(save_dir / \"y_train.npy\", y_train)\n",
    "    np.save(save_dir / \"y_test.npy\",  y_test)\n",
    "\n",
    "    metas[mode] = {\n",
    "        \"ds\": f\"{ds_id}_{name}\",\n",
    "        \"mode\": mode,\n",
    "        \"path\": str(save_dir),\n",
    "        \"train_n\": int(len(y_train)),\n",
    "        \"test_n\": int(len(y_test)),\n",
    "        \"embed_dim\": int(H_train.shape[1]),\n",
    "        \"feature_names\": feature_names,\n",
    "    }\n",
    "\n",
    "    # -------------------------\n",
    "    # (B) use_anomaly_labels\n",
    "    # -------------------------\n",
    "    mode = \"use_anomaly_labels\"\n",
    "    save_dir = base_dir / mode\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    y_for_embed = y_train  # label 사용\n",
    "\n",
    "    H_train, H_test = tabpfn_extract_embeddings(\n",
    "        X_train, y_for_embed, X_test,\n",
    "        n_fold=n_fold,\n",
    "        n_estimators=1\n",
    "    )\n",
    "\n",
    "    np.save(save_dir / \"H_train.npy\", H_train)\n",
    "    np.save(save_dir / \"H_test.npy\",  H_test)\n",
    "    np.save(save_dir / \"y_train.npy\", y_train)\n",
    "    np.save(save_dir / \"y_test.npy\",  y_test)\n",
    "\n",
    "    metas[mode] = {\n",
    "        \"ds\": f\"{ds_id}_{name}\",\n",
    "        \"mode\": mode,\n",
    "        \"path\": str(save_dir),\n",
    "        \"train_n\": int(len(y_train)),\n",
    "        \"test_n\": int(len(y_test)),\n",
    "        \"embed_dim\": int(H_train.shape[1]),\n",
    "        \"feature_names\": feature_names,\n",
    "    }\n",
    "\n",
    "    return metas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a126046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Running 4_breastw\n",
      "\n",
      ">>> Running 29_Pima\n",
      "\n",
      ">>> Running 43_WDBC\n",
      "\n",
      ">>> Running 45_wine\n",
      "\n",
      ">>> Done. Saved under: /home/haeylee/main/dataset/export_with_columns/tabpfn_embeddings/20251211_1628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'breastw': {'dummy': {'ds': '4_breastw',\n",
       "   'mode': 'dummy',\n",
       "   'path': '/home/haeylee/main/dataset/export_with_columns/tabpfn_embeddings/20251211_1628/breastw/fold5/dummy',\n",
       "   'train_n': 341,\n",
       "   'test_n': 342,\n",
       "   'embed_dim': 341,\n",
       "   'feature_names': ['Clump_Thickness',\n",
       "    'Uniformity_of_Cell_Size',\n",
       "    'Uniformity_of_Cell_Shape',\n",
       "    'Marginal_Adhesion',\n",
       "    'Single_Epithelial_Cell_Size',\n",
       "    'Bare_Nuclei',\n",
       "    'Bland_Chromatin',\n",
       "    'Normal_Nucleoli',\n",
       "    'Mitoses']},\n",
       "  'use_anomaly_labels': {'ds': '4_breastw',\n",
       "   'mode': 'use_anomaly_labels',\n",
       "   'path': '/home/haeylee/main/dataset/export_with_columns/tabpfn_embeddings/20251211_1628/breastw/fold5/use_anomaly_labels',\n",
       "   'train_n': 341,\n",
       "   'test_n': 342,\n",
       "   'embed_dim': 341,\n",
       "   'feature_names': ['Clump_Thickness',\n",
       "    'Uniformity_of_Cell_Size',\n",
       "    'Uniformity_of_Cell_Shape',\n",
       "    'Marginal_Adhesion',\n",
       "    'Single_Epithelial_Cell_Size',\n",
       "    'Bare_Nuclei',\n",
       "    'Bland_Chromatin',\n",
       "    'Normal_Nucleoli',\n",
       "    'Mitoses']}},\n",
       " 'Pima': {'dummy': {'ds': '29_Pima',\n",
       "   'mode': 'dummy',\n",
       "   'path': '/home/haeylee/main/dataset/export_with_columns/tabpfn_embeddings/20251211_1628/Pima/fold5/dummy',\n",
       "   'train_n': 384,\n",
       "   'test_n': 384,\n",
       "   'embed_dim': 384,\n",
       "   'feature_names': ['Pregnancies',\n",
       "    'Glucose',\n",
       "    'BloodPressure',\n",
       "    'SkinThickness',\n",
       "    'Insulin',\n",
       "    'BMI',\n",
       "    'DiabetesPedigreeFunction',\n",
       "    'Age']},\n",
       "  'use_anomaly_labels': {'ds': '29_Pima',\n",
       "   'mode': 'use_anomaly_labels',\n",
       "   'path': '/home/haeylee/main/dataset/export_with_columns/tabpfn_embeddings/20251211_1628/Pima/fold5/use_anomaly_labels',\n",
       "   'train_n': 384,\n",
       "   'test_n': 384,\n",
       "   'embed_dim': 384,\n",
       "   'feature_names': ['Pregnancies',\n",
       "    'Glucose',\n",
       "    'BloodPressure',\n",
       "    'SkinThickness',\n",
       "    'Insulin',\n",
       "    'BMI',\n",
       "    'DiabetesPedigreeFunction',\n",
       "    'Age']}},\n",
       " 'WDBC': {'dummy': {'ds': '43_WDBC',\n",
       "   'mode': 'dummy',\n",
       "   'path': '/home/haeylee/main/dataset/export_with_columns/tabpfn_embeddings/20251211_1628/WDBC/fold5/dummy',\n",
       "   'train_n': 183,\n",
       "   'test_n': 184,\n",
       "   'embed_dim': 183,\n",
       "   'feature_names': ['radius_mean',\n",
       "    'texture_mean',\n",
       "    'perimeter_mean',\n",
       "    'area_mean',\n",
       "    'smoothness_mean',\n",
       "    'compactness_mean',\n",
       "    'concavity_mean',\n",
       "    'concave_points_mean',\n",
       "    'symmetry_mean',\n",
       "    'fractal_dimension_mean',\n",
       "    'radius_se',\n",
       "    'texture_se',\n",
       "    'perimeter_se',\n",
       "    'area_se',\n",
       "    'smoothness_se',\n",
       "    'compactness_se',\n",
       "    'concavity_se',\n",
       "    'concave_points_se',\n",
       "    'symmetry_se',\n",
       "    'fractal_dimension_se',\n",
       "    'radius_worst',\n",
       "    'texture_worst',\n",
       "    'perimeter_worst',\n",
       "    'area_worst',\n",
       "    'smoothness_worst',\n",
       "    'compactness_worst',\n",
       "    'concavity_worst',\n",
       "    'concave_points_worst',\n",
       "    'symmetry_worst',\n",
       "    'fractal_dimension_worst']},\n",
       "  'use_anomaly_labels': {'ds': '43_WDBC',\n",
       "   'mode': 'use_anomaly_labels',\n",
       "   'path': '/home/haeylee/main/dataset/export_with_columns/tabpfn_embeddings/20251211_1628/WDBC/fold5/use_anomaly_labels',\n",
       "   'train_n': 183,\n",
       "   'test_n': 184,\n",
       "   'embed_dim': 183,\n",
       "   'feature_names': ['radius_mean',\n",
       "    'texture_mean',\n",
       "    'perimeter_mean',\n",
       "    'area_mean',\n",
       "    'smoothness_mean',\n",
       "    'compactness_mean',\n",
       "    'concavity_mean',\n",
       "    'concave_points_mean',\n",
       "    'symmetry_mean',\n",
       "    'fractal_dimension_mean',\n",
       "    'radius_se',\n",
       "    'texture_se',\n",
       "    'perimeter_se',\n",
       "    'area_se',\n",
       "    'smoothness_se',\n",
       "    'compactness_se',\n",
       "    'concavity_se',\n",
       "    'concave_points_se',\n",
       "    'symmetry_se',\n",
       "    'fractal_dimension_se',\n",
       "    'radius_worst',\n",
       "    'texture_worst',\n",
       "    'perimeter_worst',\n",
       "    'area_worst',\n",
       "    'smoothness_worst',\n",
       "    'compactness_worst',\n",
       "    'concavity_worst',\n",
       "    'concave_points_worst',\n",
       "    'symmetry_worst',\n",
       "    'fractal_dimension_worst']}},\n",
       " 'wine': {'dummy': {'ds': '45_wine',\n",
       "   'mode': 'dummy',\n",
       "   'path': '/home/haeylee/main/dataset/export_with_columns/tabpfn_embeddings/20251211_1628/wine/fold5/dummy',\n",
       "   'train_n': 64,\n",
       "   'test_n': 65,\n",
       "   'embed_dim': 64,\n",
       "   'feature_names': ['Alcohol',\n",
       "    'Malicacid',\n",
       "    'Ash',\n",
       "    'Alcalinity_of_ash',\n",
       "    'Magnesium',\n",
       "    'Total_phenols',\n",
       "    'Flavanoids',\n",
       "    'Nonflavanoid_phenols',\n",
       "    'Proanthocyanins',\n",
       "    'Color_intensity',\n",
       "    'Hue',\n",
       "    'OD280_OD315_of_diluted_wines',\n",
       "    'Proline']},\n",
       "  'use_anomaly_labels': {'ds': '45_wine',\n",
       "   'mode': 'use_anomaly_labels',\n",
       "   'path': '/home/haeylee/main/dataset/export_with_columns/tabpfn_embeddings/20251211_1628/wine/fold5/use_anomaly_labels',\n",
       "   'train_n': 64,\n",
       "   'test_n': 65,\n",
       "   'embed_dim': 64,\n",
       "   'feature_names': ['Alcohol',\n",
       "    'Malicacid',\n",
       "    'Ash',\n",
       "    'Alcalinity_of_ash',\n",
       "    'Magnesium',\n",
       "    'Total_phenols',\n",
       "    'Flavanoids',\n",
       "    'Nonflavanoid_phenols',\n",
       "    'Proanthocyanins',\n",
       "    'Color_intensity',\n",
       "    'Hue',\n",
       "    'OD280_OD315_of_diluted_wines',\n",
       "    'Proline']}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run: save for ALL datasets\n",
    "\n",
    "all_meta = {}\n",
    "\n",
    "for ds_id, name in TARGETS:\n",
    "    print(f\"\\n>>> Running {ds_id}_{name}\")\n",
    "    all_meta[name] = save_embeddings_for_one_dataset(\n",
    "        ds_id, name,\n",
    "        prefer=\"parquet\",\n",
    "        test_size=0.5,\n",
    "        seed=42,\n",
    "        n_fold=5,\n",
    "    )\n",
    "\n",
    "print(\"\\n>>> Done. Saved under:\", RUN_DIR)\n",
    "all_meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0201c12",
   "metadata": {},
   "source": [
    "- 결과보면, 샘플수 367, feature개수 30개, TabPFN 임베딩 차원 256\n",
    "- `H_train.npy`, `H_test.npy` : 원래 30차원 입력(d=30)이 256차원 임베딩(embed_dim=256)으로 변환됨\n",
    "- `y_train.npy`, `y_test.npy` : 임베딩 위에서 anomaly detector를 학습/평가하려고 저장해둠\n",
    "- 이제 임베딩(H)을 입력으로 detector를 학습하고 AUROC/AP를 계산하면 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd424072",
   "metadata": {},
   "source": [
    "### 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4ed3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_2d(H: np.ndarray, n_samples: int) -> np.ndarray:\n",
    "    H = np.asarray(H)\n",
    "\n",
    "    if H.ndim == 2:\n",
    "        return H\n",
    "\n",
    "    if H.ndim == 3:\n",
    "        # (k, n, d) 형태\n",
    "        if H.shape[1] == n_samples:\n",
    "            return H.mean(axis=0)   # k축 평균 (k=1이면 squeeze랑 같음)\n",
    "        # (n, k, d) 형태\n",
    "        if H.shape[0] == n_samples:\n",
    "            return H.mean(axis=1)\n",
    "        raise ValueError(f\"Can't align embeddings with n_samples={n_samples}, H.shape={H.shape}\")\n",
    "\n",
    "    raise ValueError(f\"Unexpected embedding ndim={H.ndim}, shape={H.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aae0428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved embeddings from RUN_DIR\n",
    "\n",
    "def load_saved_embeddings(\n",
    "    dataset_name: str,\n",
    "    n_fold: int = 5,\n",
    "    mode: str = \"dummy\",   # \"dummy\" or \"use_anomaly_labels\"\n",
    "):\n",
    "    \n",
    "    d = RUN_DIR / dataset_name / f\"fold{n_fold}\" / mode\n",
    "\n",
    "    H_train = np.load(d / \"H_train.npy\")\n",
    "    H_test  = np.load(d / \"H_test.npy\")\n",
    "    y_train = np.load(d / \"y_train.npy\").astype(int)\n",
    "    y_test  = np.load(d / \"y_test.npy\").astype(int)\n",
    "\n",
    "    H_train = to_2d(H_train, n_samples=len(y_train))\n",
    "    H_test  = to_2d(H_test,  n_samples=len(y_test))\n",
    "\n",
    "    return H_train, H_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba5c16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detector: IsolationForest + AUROC/AP\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def fit_iforest(H_train, seed=42):\n",
    "    \"\"\"H_train으로 IsolationForest 학습 (scaler 포함)\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    Htr = scaler.fit_transform(H_train)\n",
    "\n",
    "    det = IsolationForest(\n",
    "        n_estimators=400,\n",
    "        contamination=\"auto\",\n",
    "        random_state=seed,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    det.fit(Htr)\n",
    "    return scaler, det\n",
    "\n",
    "def score_iforest(scaler, det, H):\n",
    "    \"\"\"점수는 클수록 anomaly가 되도록 부호 반전\"\"\"\n",
    "    Hs = scaler.transform(H)\n",
    "    return -det.decision_function(Hs)\n",
    "\n",
    "def eval_scores(y_true, scores):\n",
    "    return {\n",
    "        \"AUROC\": float(roc_auc_score(y_true, scores)),\n",
    "        \"AP\": float(average_precision_score(y_true, scores)),\n",
    "        \"test_anom_rate\": float(y_true.mean()),\n",
    "        \"n_test\": int(len(y_true)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "981288b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>train_ds</th>\n",
       "      <th>test_ds</th>\n",
       "      <th>mode</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AP</th>\n",
       "      <th>test_anom_rate</th>\n",
       "      <th>n_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in-domain</td>\n",
       "      <td>Pima</td>\n",
       "      <td>Pima</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.575284</td>\n",
       "      <td>0.472338</td>\n",
       "      <td>0.348958</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in-domain</td>\n",
       "      <td>Pima</td>\n",
       "      <td>Pima</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>0.579940</td>\n",
       "      <td>0.432899</td>\n",
       "      <td>0.348958</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in-domain</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.934078</td>\n",
       "      <td>0.437020</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in-domain</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in-domain</td>\n",
       "      <td>breastw</td>\n",
       "      <td>breastw</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.747485</td>\n",
       "      <td>0.626270</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in-domain</td>\n",
       "      <td>breastw</td>\n",
       "      <td>breastw</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>0.778754</td>\n",
       "      <td>0.639536</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>in-domain</td>\n",
       "      <td>wine</td>\n",
       "      <td>wine</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.223117</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in-domain</td>\n",
       "      <td>wine</td>\n",
       "      <td>wine</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    scenario train_ds  test_ds                mode     AUROC        AP  \\\n",
       "0  in-domain     Pima     Pima               dummy  0.575284  0.472338   \n",
       "1  in-domain     Pima     Pima  use_anomaly_labels  0.579940  0.432899   \n",
       "2  in-domain     WDBC     WDBC               dummy  0.934078  0.437020   \n",
       "3  in-domain     WDBC     WDBC  use_anomaly_labels  1.000000  1.000000   \n",
       "4  in-domain  breastw  breastw               dummy  0.747485  0.626270   \n",
       "5  in-domain  breastw  breastw  use_anomaly_labels  0.778754  0.639536   \n",
       "6  in-domain     wine     wine               dummy  0.766667  0.223117   \n",
       "7  in-domain     wine     wine  use_anomaly_labels  1.000000  1.000000   \n",
       "\n",
       "   test_anom_rate  n_test  \n",
       "0        0.348958     384  \n",
       "1        0.348958     384  \n",
       "2        0.027174     184  \n",
       "3        0.027174     184  \n",
       "4        0.350877     342  \n",
       "5        0.350877     342  \n",
       "6        0.076923      65  \n",
       "7        0.076923      65  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-domain eval: train/test same dataset\n",
    "\n",
    "rows = []\n",
    "for _, name in TARGETS:\n",
    "    for mode in [\"dummy\", \"use_anomaly_labels\"]:\n",
    "        H_train, H_test, y_train, y_test = load_saved_embeddings(name, n_fold=5, mode=mode)\n",
    "\n",
    "        scaler, det = fit_iforest(H_train, seed=42)\n",
    "        scores = score_iforest(scaler, det, H_test)\n",
    "\n",
    "        m = eval_scores(y_test, scores)\n",
    "        rows.append({\n",
    "            \"scenario\": \"in-domain\",\n",
    "            \"train_ds\": name,\n",
    "            \"test_ds\": name,\n",
    "            \"mode\": mode,\n",
    "            **m\n",
    "        })\n",
    "\n",
    "results_in = pd.DataFrame(rows).sort_values([\"train_ds\", \"mode\"]).reset_index(drop=True)\n",
    "results_in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab4319",
   "metadata": {},
   "source": [
    "- `test_anom_rate`은 테스트셋에서 anomaly(=1) 라벨의 비율\n",
    "- WDBC / wine에서 use_anomaly_labels가 AUROC=1, AP=1\n",
    "    - TabPFN 임베딩 단계에서 라벨 정보가 들어가 분리가 과도하게 쉬워짐.\n",
    "- dummy 모드는 전체적으로 괜찮음.\n",
    "- WDBC(dummy) AUROC 0.97은 꽤 좋고, breastw(dummy) 0.78도 나쁘지 않음.\n",
    "- Pima(dummy) 0.53은 거의 랜덤에 가까움.\n",
    "- wine(dummy) AUROC 0.72인데 AP 0.16은 불균형(7.7%)에서 Precision-Recall이 어려운 케이스라 그럴 수 있음(표본도 39개로 작음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ec0e7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>train_ds</th>\n",
       "      <th>test_ds</th>\n",
       "      <th>mode</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AP</th>\n",
       "      <th>test_anom_rate</th>\n",
       "      <th>n_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>Pima</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.460573</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>Pima</td>\n",
       "      <td>breastw</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.262636</td>\n",
       "      <td>0.247679</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>Pima</td>\n",
       "      <td>wine</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>0.337302</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>Pima</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.346914</td>\n",
       "      <td>0.297775</td>\n",
       "      <td>0.350649</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>breastw</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.468463</td>\n",
       "      <td>0.383442</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>wine</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.159748</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>breastw</td>\n",
       "      <td>Pima</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.423868</td>\n",
       "      <td>0.360730</td>\n",
       "      <td>0.350649</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>breastw</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.904321</td>\n",
       "      <td>0.463054</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>breastw</td>\n",
       "      <td>wine</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.213131</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>wine</td>\n",
       "      <td>Pima</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.335638</td>\n",
       "      <td>0.275115</td>\n",
       "      <td>0.350649</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>wine</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.793210</td>\n",
       "      <td>0.570048</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>wine</td>\n",
       "      <td>breastw</td>\n",
       "      <td>dummy</td>\n",
       "      <td>0.055973</td>\n",
       "      <td>0.207951</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>Pima</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>0.635802</td>\n",
       "      <td>0.047725</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>Pima</td>\n",
       "      <td>breastw</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>0.676378</td>\n",
       "      <td>0.706066</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>Pima</td>\n",
       "      <td>wine</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>Pima</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>0.836049</td>\n",
       "      <td>0.728382</td>\n",
       "      <td>0.350649</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>breastw</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>0.990184</td>\n",
       "      <td>0.980223</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>wine</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>breastw</td>\n",
       "      <td>Pima</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>0.480494</td>\n",
       "      <td>0.362334</td>\n",
       "      <td>0.350649</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>breastw</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>breastw</td>\n",
       "      <td>wine</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>wine</td>\n",
       "      <td>Pima</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>0.826091</td>\n",
       "      <td>0.653705</td>\n",
       "      <td>0.350649</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>wine</td>\n",
       "      <td>WDBC</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cross-domain</td>\n",
       "      <td>wine</td>\n",
       "      <td>breastw</td>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>0.965356</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        scenario train_ds  test_ds                mode     AUROC        AP  \\\n",
       "0   cross-domain     Pima     WDBC               dummy  0.805556  0.460573   \n",
       "1   cross-domain     Pima  breastw               dummy  0.262636  0.247679   \n",
       "2   cross-domain     Pima     wine               dummy  0.898148  0.337302   \n",
       "3   cross-domain     WDBC     Pima               dummy  0.346914  0.297775   \n",
       "4   cross-domain     WDBC  breastw               dummy  0.468463  0.383442   \n",
       "5   cross-domain     WDBC     wine               dummy  0.722222  0.159748   \n",
       "6   cross-domain  breastw     Pima               dummy  0.423868  0.360730   \n",
       "7   cross-domain  breastw     WDBC               dummy  0.904321  0.463054   \n",
       "8   cross-domain  breastw     wine               dummy  0.805556  0.213131   \n",
       "9   cross-domain     wine     Pima               dummy  0.335638  0.275115   \n",
       "10  cross-domain     wine     WDBC               dummy  0.793210  0.570048   \n",
       "11  cross-domain     wine  breastw               dummy  0.055973  0.207951   \n",
       "12  cross-domain     Pima     WDBC  use_anomaly_labels  0.635802  0.047725   \n",
       "13  cross-domain     Pima  breastw  use_anomaly_labels  0.676378  0.706066   \n",
       "14  cross-domain     Pima     wine  use_anomaly_labels  1.000000  1.000000   \n",
       "15  cross-domain     WDBC     Pima  use_anomaly_labels  0.836049  0.728382   \n",
       "16  cross-domain     WDBC  breastw  use_anomaly_labels  0.990184  0.980223   \n",
       "17  cross-domain     WDBC     wine  use_anomaly_labels  1.000000  1.000000   \n",
       "18  cross-domain  breastw     Pima  use_anomaly_labels  0.480494  0.362334   \n",
       "19  cross-domain  breastw     WDBC  use_anomaly_labels  1.000000  1.000000   \n",
       "20  cross-domain  breastw     wine  use_anomaly_labels  1.000000  1.000000   \n",
       "21  cross-domain     wine     Pima  use_anomaly_labels  0.826091  0.653705   \n",
       "22  cross-domain     wine     WDBC  use_anomaly_labels  1.000000  1.000000   \n",
       "23  cross-domain     wine  breastw  use_anomaly_labels  0.976608  0.965356   \n",
       "\n",
       "    test_anom_rate  n_test  \n",
       "0         0.027027     111  \n",
       "1         0.351220     205  \n",
       "2         0.076923      39  \n",
       "3         0.350649     231  \n",
       "4         0.351220     205  \n",
       "5         0.076923      39  \n",
       "6         0.350649     231  \n",
       "7         0.027027     111  \n",
       "8         0.076923      39  \n",
       "9         0.350649     231  \n",
       "10        0.027027     111  \n",
       "11        0.351220     205  \n",
       "12        0.027027     111  \n",
       "13        0.351220     205  \n",
       "14        0.076923      39  \n",
       "15        0.350649     231  \n",
       "16        0.351220     205  \n",
       "17        0.076923      39  \n",
       "18        0.350649     231  \n",
       "19        0.027027     111  \n",
       "20        0.076923      39  \n",
       "21        0.350649     231  \n",
       "22        0.027027     111  \n",
       "23        0.351220     205  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-domain eval: train on A, test on B\n",
    "\n",
    "rows = []\n",
    "names = [name for _, name in TARGETS]\n",
    "\n",
    "for train_name in names:\n",
    "    for test_name in names:\n",
    "        if train_name == test_name:\n",
    "            continue\n",
    "\n",
    "        for mode in [\"dummy\", \"use_anomaly_labels\"]:\n",
    "            H_train_A, _, _, _ = load_saved_embeddings(train_name, n_fold=5, mode=mode)\n",
    "            _, H_test_B, _, y_test_B = load_saved_embeddings(test_name, n_fold=5, mode=mode)\n",
    "\n",
    "            scaler, det = fit_iforest(H_train_A, seed=42)\n",
    "            scores = score_iforest(scaler, det, H_test_B)\n",
    "\n",
    "            m = eval_scores(y_test_B, scores)\n",
    "            rows.append({\n",
    "                \"scenario\": \"cross-domain\",\n",
    "                \"train_ds\": train_name,\n",
    "                \"test_ds\": test_name,\n",
    "                \"mode\": mode,\n",
    "                **m\n",
    "            })\n",
    "\n",
    "results_cross = pd.DataFrame(rows).sort_values([\"mode\", \"train_ds\", \"test_ds\"]).reset_index(drop=True)\n",
    "results_cross\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe138d40",
   "metadata": {},
   "source": [
    "1) dummy 결과 해석\n",
    "- WDBC를 test로 두면 꽤 잘 나옴\n",
    "    - Pima→WDBC (0.81), breastw→WDBC (0.90), wine→WDBC (0.79)\n",
    "    - TabPFN 임베딩 공간에서 WDBC의 anomaly는 다른 도메인에서 학습한 정상성으로도 어느 정도 잡힌다는 신호?\n",
    "- breastw를 test로 두면 일부는 거의 반대로 나옴\n",
    "    - wine→breastw (0.056), Pima→breastw (0.263), WDBC→breastw (0.468)\n",
    "    - AUROC가 0.5보다 한참 아래면, 보통 (i) 점수 방향이 뒤집혔거나(=정상에 높은 점수를 줌), (ii) 소스에서 학습한 정상성이 타깃 도메인에서는 정반대 의미가 되어버렸거나, (iii) 단순히 도메인 시프트가 커서 실패일 가능성..\n",
    "    - AUROC는 순위 기반이라, 완전 반대로라면 1−AUROC로 뒤집으면 높아진다고 함.. (예: 0.056 → 뒤집으면 0.944)\n",
    "- representation이 도메인별로 방향/스케일이 바뀌는 문제가 있다는 힌트..?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd0500",
   "metadata": {},
   "source": [
    "2) `use_anomaly_labels cross-domain` 결과 해석.. 일단 그대로 믿으면 안 됨\n",
    "- 현재 `use_anomaly_labels` 모드에서 타깃(test) 도메인 임베딩을 만들 때도 그 도메인의 y_train 라벨을 써서 임베딩을 뽑는 상태.\n",
    "- 즉 cross-domain에서 source는 라벨 사용은 OK지만, target 임베딩이 target 라벨을 이미 사용한 상태라서\n",
    "    - WDBC→wine (1.0) 같은 값이 많은 이유가 **타깃 쪽 임베딩 단계에 정보가 섞인 효과**일 가능성이 큼.\n",
    "- 공정하게 하려면:\n",
    "    - target(test) 쪽 임베딩은 항상 dummy로 고정해야 할듯.. \n",
    "    - 비교 실험해봐야할듯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6d3facf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dummy</td>\n",
       "      <td>0.568542</td>\n",
       "      <td>0.331379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>use_anomaly_labels</td>\n",
       "      <td>0.868467</td>\n",
       "      <td>0.786983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mode     AUROC        AP\n",
       "0               dummy  0.568542  0.331379\n",
       "1  use_anomaly_labels  0.868467  0.786983"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary (cross-domain): dummy vs labels\n",
    "\n",
    "summary_cross = (results_cross\n",
    "                 .groupby(\"mode\", as_index=False)[[\"AUROC\", \"AP\"]]\n",
    "                 .mean()\n",
    "                )\n",
    "summary_cross\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775384d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405cef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aacb76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
