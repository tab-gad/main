{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65b2ff3a",
   "metadata": {},
   "source": [
    "# Tabular GAD 실험 (ADBench + TabPFN + LLM Embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad9b04",
   "metadata": {},
   "source": [
    "##### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "502845e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tabpfn_extensions import TabPFNClassifier\n",
    "from tabpfn_extensions.embedding import TabPFNEmbedding\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a7a086",
   "metadata": {},
   "source": [
    "##### 데이터셋 전처리\n",
    "- 결과 분석이나 LLM 컬럼 처리 등에서 컬럼명이 필요하므로 가져오기\n",
    "- TabPFN에서도 A/B 도메인에서 학습하고 C에서 테스트하려면 여러 도메인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71b0f375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 4_breastw | X=(683, 9) -> /home/haeylee/main/dataset/export_with_columns\n",
      "[OK] 29_Pima | X=(768, 8) -> /home/haeylee/main/dataset/export_with_columns\n",
      "[OK] 43_WDBC | X=(367, 30) -> /home/haeylee/main/dataset/export_with_columns\n",
      "[OK] 45_wine | X=(129, 13) -> /home/haeylee/main/dataset/export_with_columns\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "ADBENCH_ROOT = Path(\"/home/haeylee/main/dataset\")\n",
    "CLASSICAL_DIR = ADBENCH_ROOT / \"Classical\"\n",
    "OUT_DIR = ADBENCH_ROOT / \"export_with_columns\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Column name maps\n",
    "COLS = {\n",
    "    \"breastw\": [\n",
    "        \"Clump_Thickness\",\"Uniformity_of_Cell_Size\",\"Uniformity_of_Cell_Shape\",\n",
    "        \"Marginal_Adhesion\",\"Single_Epithelial_Cell_Size\",\"Bare_Nuclei\",\n",
    "        \"Bland_Chromatin\",\"Normal_Nucleoli\",\"Mitoses\",\n",
    "    ],\n",
    "    \"Pima\": [\n",
    "        \"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n",
    "        \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\",\n",
    "    ],\n",
    "    \"wine\": [\n",
    "        \"Alcohol\",\"Malicacid\",\"Ash\",\"Alcalinity_of_ash\",\"Magnesium\",\n",
    "        \"Total_phenols\",\"Flavanoids\",\"Nonflavanoid_phenols\",\"Proanthocyanins\",\n",
    "        \"Color_intensity\",\"Hue\",\"OD280_OD315_of_diluted_wines\",\"Proline\",\n",
    "    ],\n",
    "    \"WDBC\": [\n",
    "        \"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\",\n",
    "        \"compactness_mean\",\"concavity_mean\",\"concave_points_mean\",\"symmetry_mean\",\"fractal_dimension_mean\",\n",
    "        \"radius_se\",\"texture_se\",\"perimeter_se\",\"area_se\",\"smoothness_se\",\n",
    "        \"compactness_se\",\"concavity_se\",\"concave_points_se\",\"symmetry_se\",\"fractal_dimension_se\",\n",
    "        \"radius_worst\",\"texture_worst\",\"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\n",
    "        \"compactness_worst\",\"concavity_worst\",\"concave_points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Datasets to export\n",
    "TARGETS = [\n",
    "    (4,  \"breastw\"),\n",
    "    (29, \"Pima\"),\n",
    "    (43, \"WDBC\"),\n",
    "    (45, \"wine\"),\n",
    "]\n",
    "\n",
    "def make_columns(d: int, preferred: list[str] | None) -> list[str]:\n",
    "\n",
    "    if preferred is not None and len(preferred) == d:\n",
    "        return preferred\n",
    "    return [f\"f{i+1}\" for i in range(d)]\n",
    "\n",
    "def export_one(ds_id: int, name: str):\n",
    "    npz_path = CLASSICAL_DIR / f\"{ds_id}_{name}.npz\"\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "    X = data[\"X\"]\n",
    "    y = data[\"y\"].astype(int)\n",
    "\n",
    "    cols = make_columns(X.shape[1], COLS.get(name))\n",
    "    df = pd.DataFrame(X, columns=cols)\n",
    "    df[\"is_anomaly\"] = y\n",
    "\n",
    "    df.to_parquet(OUT_DIR / f\"{ds_id}_{name}.parquet\", index=False)\n",
    "    df.to_csv(OUT_DIR / f\"{ds_id}_{name}.csv\", index=False)\n",
    "\n",
    "    print(f\"[OK] {ds_id}_{name} | X={X.shape} -> {OUT_DIR}\")\n",
    "\n",
    "for ds_id, name in TARGETS:\n",
    "    export_one(ds_id, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a43824fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(367, 31)\n",
      "Index(['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
      "       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
      "       'concave_points_mean'],\n",
      "      dtype='object')\n",
      "is_anomaly\n",
      "0    357\n",
      "1     10\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>is_anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.30</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.1082</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.1683</td>\n",
       "      <td>0.08751</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>...</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.3</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.6110</td>\n",
       "      <td>0.6335</td>\n",
       "      <td>0.2024</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.09876</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.87</td>\n",
       "      <td>16.67</td>\n",
       "      <td>98.64</td>\n",
       "      <td>682.5</td>\n",
       "      <td>0.1162</td>\n",
       "      <td>0.1649</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>0.08923</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.06768</td>\n",
       "      <td>...</td>\n",
       "      <td>27.37</td>\n",
       "      <td>127.1</td>\n",
       "      <td>1095.0</td>\n",
       "      <td>0.1878</td>\n",
       "      <td>0.4480</td>\n",
       "      <td>0.4704</td>\n",
       "      <td>0.2027</td>\n",
       "      <td>0.3585</td>\n",
       "      <td>0.10650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.05</td>\n",
       "      <td>19.08</td>\n",
       "      <td>113.40</td>\n",
       "      <td>895.0</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>0.1572</td>\n",
       "      <td>0.1910</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.06325</td>\n",
       "      <td>...</td>\n",
       "      <td>24.89</td>\n",
       "      <td>133.5</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.3934</td>\n",
       "      <td>0.5018</td>\n",
       "      <td>0.2543</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.09061</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        15.30         25.27          102.40      732.4           0.1082   \n",
       "1        14.87         16.67           98.64      682.5           0.1162   \n",
       "2        17.05         19.08          113.40      895.0           0.1141   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave_points_mean  symmetry_mean  \\\n",
       "0            0.1697          0.1683              0.08751         0.1926   \n",
       "1            0.1649          0.1690              0.08923         0.2157   \n",
       "2            0.1572          0.1910              0.10900         0.2131   \n",
       "\n",
       "   fractal_dimension_mean  ...  texture_worst  perimeter_worst  area_worst  \\\n",
       "0                 0.06540  ...          36.71            149.3      1269.0   \n",
       "1                 0.06768  ...          27.37            127.1      1095.0   \n",
       "2                 0.06325  ...          24.89            133.5      1189.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave_points_worst  \\\n",
       "0            0.1641             0.6110           0.6335                0.2024   \n",
       "1            0.1878             0.4480           0.4704                0.2027   \n",
       "2            0.1703             0.3934           0.5018                0.2543   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  is_anomaly  \n",
       "0          0.4027                  0.09876           1  \n",
       "1          0.3585                  0.10650           1  \n",
       "2          0.3109                  0.09061           1  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 예시 봐보기\n",
    "\n",
    "EXPORT_DIR = ADBENCH_ROOT / \"export_with_columns\"\n",
    "\n",
    "def load_exported_df(ds_id: int, name: str, prefer: str = \"parquet\") -> pd.DataFrame:\n",
    "    \"\"\"export_with_columns에서 DataFrame을 읽어오기\"\"\"\n",
    "    p_parq = EXPORT_DIR / f\"{ds_id}_{name}.parquet\"\n",
    "    p_csv  = EXPORT_DIR / f\"{ds_id}_{name}.csv\"\n",
    "\n",
    "    if prefer == \"parquet\" and p_parq.exists():\n",
    "        return pd.read_parquet(p_parq)\n",
    "    if prefer == \"csv\" and p_csv.exists():\n",
    "        return pd.read_csv(p_csv)\n",
    "\n",
    "    # fallback\n",
    "    return pd.read_parquet(p_parq) if p_parq.exists() else pd.read_csv(p_csv)\n",
    "\n",
    "def df_to_xy(df: pd.DataFrame):\n",
    "    \"\"\"DataFrame -> (X, y, feature_names)로 변환\"\"\"\n",
    "    y = df[\"is_anomaly\"].to_numpy().astype(int) \n",
    "    X = df.drop(columns=[\"is_anomaly\"]).to_numpy() # anomaly 라벨 제외한 feature 행렬\n",
    "    feature_names = df.columns.drop(\"is_anomaly\").tolist() # 결과해석용으로 컬럼명도 뽑기\n",
    "    return X, y, feature_names\n",
    "\n",
    "# 예시: WDBC 확인\n",
    "df_wdbc = load_exported_df(43, \"WDBC\", prefer=\"parquet\")\n",
    "print(df_wdbc.shape)\n",
    "print(df_wdbc.columns[:8])\n",
    "print(df_wdbc[\"is_anomaly\"].value_counts())\n",
    "\n",
    "df_wdbc.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9540a17c",
   "metadata": {},
   "source": [
    "### TabPFN (1) 임베딩 기반 파이프라인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ddfe01",
   "metadata": {},
   "source": [
    "- 각 row를 벡터 임베딩으로 만들고, 그 임베딩 위에 anomaly detector 얹어 AD 성능 구하는 코드\n",
    "- TabPFN 임베딩 추출은 `tabpfn-extensions`의 Embeddings extension 그대로 씀\n",
    "- 흐름\n",
    "1) ADBench 데이터 로드\n",
    "2) train/test split\n",
    "3) TabPFNEmbedding으로 H_train, H_test 추출\n",
    "4) 임베딩에 대해 간단한 detector(IsolationForest) 학습 - test에서 anomaly score\n",
    "5) AD 성능 계산 (AUROC / Average Precision)\n",
    "- 라벨을 임베딩 추출에 쓸지말지 둘다 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c11e1",
   "metadata": {},
   "source": [
    "##### TabPFN을 임베딩 추출기로 사용해 row embedding 뽑기\n",
    "- n_fold = 0 : 빠른 대신 덜 robust (vanilla)\n",
    "- n_fold > 0 : CV 기반 임베딩 (default:5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f035df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabPFNEmbedding으로 H_train, H_test 뽑기\n",
    "\n",
    "def tabpfn_extract_embeddings(\n",
    "    X_train, y_train_for_embed, X_test,\n",
    "    n_fold=5,\n",
    "    n_estimators=1,\n",
    "):\n",
    "    \n",
    "    clf = TabPFNClassifier(n_estimators=n_estimators)\n",
    "    embedder = TabPFNEmbedding(tabpfn_clf=clf, n_fold=n_fold)\n",
    "\n",
    "    # n_fold=0일 때만 fit 필요\n",
    "    if n_fold == 0:\n",
    "        embedder.fit(X_train, y_train_for_embed)\n",
    "\n",
    "    H_train = embedder.get_embeddings(X_train, y_train_for_embed, X_test, data_source=\"train\")\n",
    "    H_test  = embedder.get_embeddings(X_train, y_train_for_embed, X_test, data_source=\"test\")\n",
    "    return H_train, H_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39077a25",
   "metadata": {},
   "source": [
    "- 하나의 데이터셋에 대해 split - 임베딩 - 저장하기\n",
    "    1) export된 데이터 로드\n",
    "    2) train/test split\n",
    "    3) TabPFN 임베딩 추출 (H_train, H_test)\n",
    "    4) npy로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d93ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# (1) Save directories\n",
    "# =========================\n",
    "\n",
    "EMB_OUT_DIR = EXPORT_DIR / \"tabpfn_embeddings\"\n",
    "EMB_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = EMB_OUT_DIR / RUN_ID\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\">>> New run directory:\", RUN_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4660b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_OUT_DIR = EXPORT_DIR / \"tabpfn_embeddings\"\n",
    "EMB_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run_one_dataset_tabpfn_embedding(\n",
    "    ds_id: int, name: str,\n",
    "    prefer=\"parquet\",\n",
    "    test_size=0.3,\n",
    "    seed=42,\n",
    "    max_n=4000,\n",
    "    n_fold=5,\n",
    "    label_mode=\"dummy\",  # \"dummy\" or \"use_anomaly_labels\"\n",
    "):\n",
    "\n",
    "    df = load_exported_df(ds_id, name, prefer=prefer)\n",
    "    X, y, feature_names = df_to_xy(df)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=y # 아노말리 비율 맞추면서 나눔\n",
    "    )\n",
    "\n",
    "    # 임베딩 추출에 쓸 y\n",
    "    if label_mode == \"dummy\":\n",
    "        y_for_embed = np.zeros_like(y_train)\n",
    "    elif label_mode == \"use_anomaly_labels\":\n",
    "        y_for_embed = y_train\n",
    "    else:\n",
    "        raise ValueError(\"label_mode must be 'dummy' or 'use_anomaly_labels'\")\n",
    "\n",
    "    H_train, H_test = tabpfn_extract_embeddings(\n",
    "        X_train, y_for_embed, X_test,\n",
    "        n_fold=n_fold,\n",
    "        n_estimators=1\n",
    "    )\n",
    "\n",
    "    prefix = f\"{ds_id}_{name}_fold{n_fold}_{label_mode}\"\n",
    "    np.save(EMB_OUT_DIR / f\"{prefix}_H_train.npy\", H_train)\n",
    "    np.save(EMB_OUT_DIR / f\"{prefix}_H_test.npy\",  H_test)\n",
    "    np.save(EMB_OUT_DIR / f\"{prefix}_y_train.npy\", y_train)\n",
    "    np.save(EMB_OUT_DIR / f\"{prefix}_y_test.npy\",  y_test)\n",
    "\n",
    "    meta = {\n",
    "        \"ds\": f\"{ds_id}_{name}\",\n",
    "        \"n\": int(X.shape[0]),\n",
    "        \"d\": int(X.shape[1]),\n",
    "        \"embed_dim\": int(H_train.shape[1]),\n",
    "        \"n_fold\": int(n_fold),\n",
    "        \"label_mode\": label_mode,\n",
    "        \"saved_prefix\": prefix,\n",
    "        \"feature_names\": feature_names,\n",
    "    }\n",
    "    return meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4176489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ds': '43_WDBC', 'n': 367, 'd': 30, 'embed_dim': 256, 'n_fold': 5, 'label_mode': 'dummy', 'saved_prefix': '43_WDBC_fold5_dummy', 'feature_names': ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave_points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave_points_worst', 'symmetry_worst', 'fractal_dimension_worst']}\n"
     ]
    }
   ],
   "source": [
    "meta = run_one_dataset_tabpfn_embedding(\n",
    "    43, \"WDBC\",\n",
    "    label_mode=\"dummy\",\n",
    "    n_fold=5,\n",
    "    max_n=4000\n",
    ")\n",
    "print(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0201c12",
   "metadata": {},
   "source": [
    "- 결과보면, 샘플수 367, feature개수 30개, TabPFN 임베딩 차원 256\n",
    "- `H_train.npy`, `H_test.npy` : 원래 30차원 입력(d=30)이 256차원 임베딩(embed_dim=256)으로 변환됨\n",
    "- `y_train.npy`, `y_test.npy` : 임베딩 위에서 anomaly detector를 학습/평가하려고 저장해둠\n",
    "- 이제 임베딩(H)을 입력으로 detector를 학습하고 AUROC/AP를 계산하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db02d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy, use_labels 둘다 임베딩 저장 (4개 전체)\n",
    "\n",
    "def save_embeddings_both_modes(ds_id: int, name: str, n_fold=5, seed=42, test_size=0.3, prefer=\"parquet\"):\n",
    "    \"\"\"같은 split으로 dummy / use_anomaly_labels 임베딩 둘 다 저장\"\"\"\n",
    "    meta_dummy = run_one_dataset_tabpfn_embedding(\n",
    "        ds_id, name,\n",
    "        prefer=prefer,\n",
    "        test_size=test_size,\n",
    "        seed=seed,\n",
    "        n_fold=n_fold,\n",
    "        label_mode=\"dummy\",\n",
    "    )\n",
    "    meta_lbl = run_one_dataset_tabpfn_embedding(\n",
    "        ds_id, name,\n",
    "        prefer=prefer,\n",
    "        test_size=test_size,\n",
    "        seed=seed,\n",
    "        n_fold=n_fold,\n",
    "        label_mode=\"use_anomaly_labels\",\n",
    "    )\n",
    "    return meta_dummy, meta_lbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9ed545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] 4_breastw_fold5_dummy\n",
      "[saved] 4_breastw_fold5_use_anomaly_labels\n",
      "[saved] 29_Pima_fold5_dummy\n",
      "[saved] 29_Pima_fold5_use_anomaly_labels\n",
      "[saved] 43_WDBC_fold5_dummy\n",
      "[saved] 43_WDBC_fold5_use_anomaly_labels\n",
      "[saved] 45_wine_fold5_dummy\n",
      "[saved] 45_wine_fold5_use_anomaly_labels\n"
     ]
    }
   ],
   "source": [
    "metas = []\n",
    "for ds_id, name in TARGETS:\n",
    "    md, ml = save_embeddings_both_modes(ds_id, name, n_fold=5, seed=42, test_size=0.3)\n",
    "    metas.append(md)\n",
    "    metas.append(ml)\n",
    "    print(\"[saved]\", md[\"saved_prefix\"])\n",
    "    print(\"[saved]\", ml[\"saved_prefix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae0428b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c16c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
